'''
4.	Develop a logistic regression-like algorithm for the following cost function.

Y = 1 - Cost function goes from 100 to 0 linearly as hypothesis function goes from 0 to 1

Y = 0 - Cost function goes from 0 to 100 linearly as hypothesis function goes from 0 to 1

Compare results with those of the standard logistic algorithm.
'''

hθ(x) = g(θtX)
g(z) = 1 / ( 1 + exp(−z) )
g’(z) = g(z)*(1-g(z)).


d/dθj[ hθ(X) ] = d/dθ [ 1 / ( 1 + exp( -θXij))]

d/dθj [-( 1 + exp(-θXj) )] / ( 1 + exp(-θXj) ) ** 2

- d/dθj [-θX ] * exp(-θXj) / ( 1 + exp(-θXj) ) ** 2

d/dθj[ hθ(x) ] = Xj * exp(-θX) / ( 1 + exp(-θX) ) ** 2

J(θ) = Cost = 100 - 100 * hθ(X)         when Y = 1
              100 * hθ(X)               when Y = 0
              
J(θ) = Yi * ( 100 - 100 * hθ(xi) ) + ( 1 - Yi ) * ( 100 * hθ(xi) )

dJ / dθj = 1 / m * sum i=1 to m(d/dθj[ Yi * ( 100 - 100 * hθ(xi) ) + ( 1 - Yi ) * ( 100 * hθ(xi) ) ] )

1 / m * sum i=1 to m( Yi * d/dθj[ ( 100 - 100 * hθ(xi) ) ] + ( 1 - Yi ) * d/dθj[ ( 100 * hθ(xi) ) ] )
 
1 / m * sum i=1 to m( 100 * Yi * d/dθj[ ( 1 - hθ(xi) ) ] + ( 100 - 100 * Yi ) * d/dθj[ ( hθ(xi) ) ] )
 
1 / m * sum i=1 to m( -100 * Yi * d/dθj[ ( hθ(xi) ) ] + ( 100 - 100 * Yi ) * d/dθj[ ( hθ(xi) ) ] ) 
 
1 / m * sum i=1 to m( ( 100 - 200 * Yi ) * d/dθj[ ( hθ(Xi) ) ] )

1 / m * sum i=1 to m( ( 100 - 200 * Yi ) * d/dθj[ ( g(θtX) ) ] )

1 / m * sum i=1 to m( ( 100 - 200 * Yi ) *  ( g(θtX) * ( 1 - g(θtX) ) *  d/dθj[ θtXi ] )

dJ / dθj = 1 / m * sum i=1 to m( ( 100 - 200 * Yi ) *  ( hθ(Xi) * ( 1 -  hθ(Xi) ) *  Xij  )

θj = θj – α * dJ / dθj 

θ0 = θ0 – α *  1 / m * sum i=1 to m( ( 100 - 200 * Yi ) *  ( hθ(Xi) * ( 1 -  hθ(Xi) ) )

θj = θj – α *  1 / m * sum i=1 to m( ( 100 - 200 * Yi ) *  ( hθ(Xi) * ( 1 -  hθ(Xi) ) *  Xij  ); j = 1,2,..,n 

Algorithm: 
Step 1: Select an initial θ(1). 
Step 2: During iteration k
        θ(k+1) = θ(k) – α * dJ / dθ 
        k=k+1
Step 3: If accuracy is acceptable, stop.  Otherwise, go to Step 2. 


*********Need to think about this******
One thing that stands out is that the gradient will be constant unlike when you 
use the standard logistic algorithm. Since we have only used a constant learning 
rate we are dependent on the gradient magnitude to adjust the rate we update 
theta. Since there will be no change in the adjustment magnitude to theta we 
will require a smaller learning rate to ensure we do not overshoot the minimum 
to early because once that happens we will not be able to lower the cost because 
we will just go back and forth between those two cost.
